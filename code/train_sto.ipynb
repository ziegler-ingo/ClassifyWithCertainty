{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da02785-441a-4fba-b17e-dcd54594bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from utils import (\n",
    "    seed_everything,\n",
    "    trainable_model_params,\n",
    "    total_model_params,\n",
    "    save_train_metrics,\n",
    "    process_train_eval,\n",
    "    Loader\n",
    ")\n",
    "\n",
    "from transformer import TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e39f5e-e9cd-493c-8b0f-ed1ce8a33018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_everything(1234)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = 'cpu'\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor('spm_wiki.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22a7db-32e4-4a37-bc23-14900ed0050f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# In Domain: 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15110b43-a5c2-4397-ac81-c7b460d3feb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 512\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "N_EPOCHS = 50\n",
    "LR = 1e-3\n",
    "\n",
    "loader = Loader(batch_size=BATCH_SIZE, max_len=MAX_LEN, num_workers=NUM_WORKERS)\n",
    "train_loader = loader.load('20news', 'train')\n",
    "val_loader = loader.load('20news', 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a35c2-1f32-4d48-a5e4-dc71b2b81854",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Without Spectral Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a8dbc-88f4-4653-a21a-43a4e4d3cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_20ng = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'emb_dim': 1024,\n",
    "    'n_layers': 1,\n",
    "    'n_heads': 8,\n",
    "    'forward_dim': 1024,\n",
    "    'dropout': 0.2,\n",
    "    'max_len': MAX_LEN,\n",
    "    'pad_idx': tokenizer.pad_id(),\n",
    "    'kind': 'sto',\n",
    "    'tau2': 10,\n",
    "    'spectral': False,\n",
    "    'n_classes': 20,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "params_to_save = {\n",
    "    'lr': LR,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'optim': 'adam'\n",
    "}\n",
    "\n",
    "model = TransformerEncoder(**params_20ng).to(device)\n",
    "print(f\"Total model params: {total_model_params(model):,d}\")\n",
    "print(f\"Trainable model params: {trainable_model_params(model):,d}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17faed4-6d67-423a-b7cd-e2f5e3934e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '../../../params/udl/sto_20news/'\n",
    "now = datetime.now()\n",
    "path_now = path + now.strftime(\"%Y-%m-%d %H:%M:%S\") + '/'\n",
    "os.makedirs(path_now)\n",
    "logging_path = path_now + 'results.csv'\n",
    "\n",
    "for k, v in params_20ng.items():\n",
    "    params_to_save[k] = v\n",
    "\n",
    "with open(path_now + 'params.json', 'w') as f:\n",
    "    json.dump(params_to_save, f, indent=4)\n",
    "    \n",
    "highest_val_acc = 0\n",
    "\n",
    "# normal training loop\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    if epoch == 10:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 1e-4\n",
    "            print('updated learning rate')\n",
    "    model.train()\n",
    "    train_loss, train_acc = process_train_eval(\n",
    "        model, train_loader, criterion, optimizer\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc = process_train_eval(model, val_loader, criterion)\n",
    "        \n",
    "    # save metrics\n",
    "    save_train_metrics(\n",
    "        epoch,\n",
    "        train_loss,\n",
    "        train_acc,\n",
    "        val_loss,\n",
    "        val_acc,\n",
    "        path=logging_path,\n",
    "    )\n",
    "    \n",
    "\n",
    "    if val_acc > highest_val_acc:\n",
    "        highest_val_acc = val_acc\n",
    "        _path = path_now + f\"acc{val_acc:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), _path)\n",
    "\n",
    "    print(\n",
    "        f\"Training:   [Epoch {epoch:2d}, Loss: {train_loss:8.6f}, Acc: {train_acc:.6f}]\"\n",
    "    )\n",
    "    print(f\"Evaluation: [Epoch {epoch:2d}, Loss: {val_loss:8.6f}, Acc: {val_acc:.6f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a084a5e-2037-423f-aa7a-ea9ca60d65f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## With Spectral Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04edd4de-b9c7-4d9b-9826-a151dbd83baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_20ng_sn = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'emb_dim': 1024,\n",
    "    'n_layers': 1,\n",
    "    'n_heads': 8,\n",
    "    'forward_dim': 1024,\n",
    "    'dropout': 0.2,\n",
    "    'max_len': MAX_LEN,\n",
    "    'pad_idx': tokenizer.pad_id(),\n",
    "    'kind': 'sto',\n",
    "    'tau2': 40,\n",
    "    'spectral': True,\n",
    "    'n_classes': 20,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "params_to_save = {\n",
    "    'lr': LR,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'optim': 'adam'\n",
    "}\n",
    "\n",
    "model = TransformerEncoder(**params_20ng_sn).to(device)\n",
    "print(f\"Total model params: {total_model_params(model):,d}\")\n",
    "print(f\"Trainable model params: {trainable_model_params(model):,d}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb2398-29d2-46bf-a3d6-0c20303a0c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '../../../params/udl/sto_20news_sn/'\n",
    "now = datetime.now()\n",
    "path_now = path + now.strftime(\"%Y-%m-%d %H:%M:%S\") + '/'\n",
    "os.makedirs(path_now)\n",
    "logging_path = path_now + 'results.csv'\n",
    "\n",
    "for k, v in params_20ng_sn.items():\n",
    "    params_to_save[k] = v\n",
    "\n",
    "with open(path_now + 'params.json', 'w') as f:\n",
    "    json.dump(params_to_save, f, indent=4)\n",
    "    \n",
    "highest_val_acc = 0\n",
    "\n",
    "# normal training loop\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    if epoch == 10:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 1e-4\n",
    "            print('updated learning rate')\n",
    "    model.train()\n",
    "    train_loss, train_acc = process_train_eval(\n",
    "        model, train_loader, criterion, optimizer\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc = process_train_eval(model, val_loader, criterion)\n",
    "        \n",
    "    # save metrics\n",
    "    save_train_metrics(\n",
    "        epoch,\n",
    "        train_loss,\n",
    "        train_acc,\n",
    "        val_loss,\n",
    "        val_acc,\n",
    "        path=logging_path,\n",
    "    )\n",
    "    \n",
    "\n",
    "    if val_acc > highest_val_acc:\n",
    "        highest_val_acc = val_acc\n",
    "        _path = path_now + f\"acc{val_acc:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), _path)\n",
    "\n",
    "    print(\n",
    "        f\"Training:   [Epoch {epoch:2d}, Loss: {train_loss:8.6f}, Acc: {train_acc:.6f}]\"\n",
    "    )\n",
    "    print(f\"Evaluation: [Epoch {epoch:2d}, Loss: {val_loss:8.6f}, Acc: {val_acc:.6f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51245d-381f-49a8-a002-2d7d31594847",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# In Domain: TREC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756371b-9d89-48d5-b20f-1f133f7bce03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 512\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "N_EPOCHS = 100\n",
    "LR = 1e-3\n",
    "\n",
    "loader = Loader(batch_size=BATCH_SIZE, max_len=MAX_LEN, num_workers=NUM_WORKERS)\n",
    "train_loader = loader.load('trec', 'train')\n",
    "val_loader = loader.load('trec', 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e7086-ebd3-4cbe-88f3-fc2e390ef3fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Without Spectral Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92aff8c-5868-4d77-addf-c9e4e1a26efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_trec = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'emb_dim': 1024,\n",
    "    'n_layers': 1,\n",
    "    'n_heads': 8,\n",
    "    'forward_dim': 1024,\n",
    "    'dropout': 0.1,\n",
    "    'max_len': MAX_LEN,\n",
    "    'pad_idx': tokenizer.pad_id(),\n",
    "    'kind': 'sto',\n",
    "    'tau2': 10,\n",
    "    'spectral': False,\n",
    "    'n_classes': 50,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "params_to_save = {\n",
    "    'lr': LR,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'optim': 'adam'\n",
    "}\n",
    "\n",
    "model = TransformerEncoder(**params_trec).to(device)\n",
    "print(f\"Total model params: {total_model_params(model):,d}\")\n",
    "print(f\"Trainable model params: {trainable_model_params(model):,d}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103e512-a79f-4c7e-bd1d-4ee89db28054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '../../../params/udl/sto_trec/'\n",
    "now = datetime.now()\n",
    "path_now = path + now.strftime(\"%Y-%m-%d %H:%M:%S\") + '/'\n",
    "os.makedirs(path_now)\n",
    "logging_path = path_now + 'results.csv'\n",
    "\n",
    "for k, v in params_trec.items():\n",
    "    params_to_save[k] = v\n",
    "\n",
    "with open(path_now + 'params.json', 'w') as f:\n",
    "    json.dump(params_to_save, f, indent=4)\n",
    "\n",
    "highest_val_acc = 0\n",
    "\n",
    "# normal training loop\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    if epoch == 50:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 1e-4\n",
    "            print('updated learning rate')\n",
    "    model.train()\n",
    "    train_loss, train_acc = process_train_eval(\n",
    "        model, train_loader, criterion, optimizer\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc = process_train_eval(model, val_loader, criterion)\n",
    "\n",
    "    # save metrics\n",
    "    save_train_metrics(\n",
    "        epoch,\n",
    "        train_loss,\n",
    "        train_acc,\n",
    "        val_loss,\n",
    "        val_acc,\n",
    "        path=logging_path,\n",
    "    )\n",
    "\n",
    "\n",
    "    if val_acc > highest_val_acc:\n",
    "        highest_val_acc = val_acc\n",
    "        _path = path_now + f\"acc{val_acc:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), _path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b85ec-c2b2-4103-ac0b-5970a56e02c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## With Spectral Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd96c4-19c8-4999-9346-e606cdb76e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_trec_sn = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'emb_dim': 1024,\n",
    "    'n_layers': 1,\n",
    "    'n_heads': 8,\n",
    "    'forward_dim': 1024,\n",
    "    'dropout': 0.1,\n",
    "    'max_len': MAX_LEN,\n",
    "    'pad_idx': tokenizer.pad_id(),\n",
    "    'kind': 'sto',\n",
    "    'tau2': 10,\n",
    "    'spectral': True,\n",
    "    'n_classes': 50,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "params_to_save = {\n",
    "    'lr': LR,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'optim': 'adam'\n",
    "}\n",
    "\n",
    "model = TransformerEncoder(**params_trec_sn).to(device)\n",
    "print(f\"Total model params: {total_model_params(model):,d}\")\n",
    "print(f\"Trainable model params: {trainable_model_params(model):,d}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea19bc-0641-4f93-8645-25ca36f24d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '../../../params/udl/sto_trec_sn/'\n",
    "now = datetime.now()\n",
    "path_now = path + now.strftime(\"%Y-%m-%d %H:%M:%S\") + '/'\n",
    "os.makedirs(path_now)\n",
    "logging_path = path_now + 'results.csv'\n",
    "\n",
    "for k, v in params_trec_sn.items():\n",
    "    params_to_save[k] = v\n",
    "\n",
    "with open(path_now + 'params.json', 'w') as f:\n",
    "    json.dump(params_to_save, f, indent=4)\n",
    "\n",
    "highest_val_acc = 0\n",
    "\n",
    "# normal training loop\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    if epoch == 50:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 1e-4\n",
    "            print('updated learning rate')\n",
    "    model.train()\n",
    "    train_loss, train_acc = process_train_eval(\n",
    "        model, train_loader, criterion, optimizer\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc = process_train_eval(model, val_loader, criterion)\n",
    "\n",
    "    # save metrics\n",
    "    save_train_metrics(\n",
    "        epoch,\n",
    "        train_loss,\n",
    "        train_acc,\n",
    "        val_loss,\n",
    "        val_acc,\n",
    "        path=logging_path,\n",
    "    )\n",
    "\n",
    "\n",
    "    if val_acc > highest_val_acc:\n",
    "        highest_val_acc = val_acc\n",
    "        _path = path_now + f\"acc{val_acc:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), _path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7740d8b8-e3c6-4a10-9a54-ada6fc92d5b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# In Domain: SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7945a-f64e-457a-bc5e-99f4a016b345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 512\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "N_EPOCHS = 50\n",
    "LR = 1e-4\n",
    "\n",
    "loader = Loader(batch_size=BATCH_SIZE, max_len=MAX_LEN, num_workers=NUM_WORKERS)\n",
    "train_loader = loader.load('sst', 'train')\n",
    "val_loader = loader.load('sst', 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070467cb-6778-4287-8d65-b165304a9769",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Without Spectral Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24526ebf-0016-4111-9561-2758733d29ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_sst = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'emb_dim': 1024,\n",
    "    'n_layers': 1,\n",
    "    'n_heads': 8,\n",
    "    'forward_dim': 2048,\n",
    "    'dropout': 0.5,\n",
    "    'max_len': MAX_LEN,\n",
    "    'pad_idx': tokenizer.pad_id(),\n",
    "    'kind': 'sto',\n",
    "    'tau2': 40,\n",
    "    'spectral': False,\n",
    "    'n_classes': 2,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "params_to_save = {\n",
    "    'lr': LR,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'optim': 'adam'\n",
    "}\n",
    "\n",
    "model = TransformerEncoder(**params_sst).to(device)\n",
    "print(f\"Total model params: {total_model_params(model):,d}\")\n",
    "print(f\"Trainable model params: {trainable_model_params(model):,d}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa97865-8144-4a81-85df-922db3474594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '../../../params/udl/sto_sst/'\n",
    "now = datetime.now()\n",
    "path_now = path + now.strftime(\"%Y-%m-%d %H:%M:%S\") + '/'\n",
    "os.makedirs(path_now)\n",
    "logging_path = path_now + 'results.csv'\n",
    "\n",
    "for k, v in params_sst.items():\n",
    "    params_to_save[k] = v\n",
    "\n",
    "with open(path_now + 'params.json', 'w') as f:\n",
    "    json.dump(params_to_save, f, indent=4)\n",
    "\n",
    "highest_val_acc = 0\n",
    "\n",
    "# normal training loop\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    if epoch == 30:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 5e-5\n",
    "            print('updated learning rate')\n",
    "    model.train()\n",
    "    train_loss, train_acc = process_train_eval(\n",
    "        model, train_loader, criterion, optimizer\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc = process_train_eval(model, val_loader, criterion)\n",
    "\n",
    "    # save metrics\n",
    "    save_train_metrics(\n",
    "        epoch,\n",
    "        train_loss,\n",
    "        train_acc,\n",
    "        val_loss,\n",
    "        val_acc,\n",
    "        path=logging_path,\n",
    "    )\n",
    "\n",
    "\n",
    "    if val_acc > highest_val_acc:\n",
    "        highest_val_acc = val_acc\n",
    "        _path = path_now + f\"acc{val_acc:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), _path)\n",
    "        \n",
    "    print(\n",
    "    f\"Training:   [Epoch {epoch:2d}, Loss: {train_loss:8.6f}, Acc: {train_acc:.6f}]\"\n",
    "    )\n",
    "    print(f\"Evaluation: [Epoch {epoch:2d}, Loss: {val_loss:8.6f}, Acc: {val_acc:.6f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db641c-831a-4ab5-8569-1375ac6309f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## With Spectral Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57158291-ef11-4b30-9a87-e24f98957510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_sst_sn = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'emb_dim': 1024,\n",
    "    'n_layers': 1,\n",
    "    'n_heads': 8,\n",
    "    'forward_dim': 2048,\n",
    "    'dropout': 0.4,\n",
    "    'max_len': MAX_LEN,\n",
    "    'pad_idx': tokenizer.pad_id(),\n",
    "    'kind': 'sto',\n",
    "    'tau2': 40,\n",
    "    'spectral': True,\n",
    "    'n_classes': 2,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "params_to_save = {\n",
    "    'lr': LR,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'optim': 'adam'\n",
    "}\n",
    "\n",
    "model = TransformerEncoder(**params_sst_sn).to(device)\n",
    "print(f\"Total model params: {total_model_params(model):,d}\")\n",
    "print(f\"Trainable model params: {trainable_model_params(model):,d}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db0c61-c7e8-4804-bff0-c4cb785d1dd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '../../../params/udl/sto_sst_sn/'\n",
    "now = datetime.now()\n",
    "path_now = path + now.strftime(\"%Y-%m-%d %H:%M:%S\") + '/'\n",
    "os.makedirs(path_now)\n",
    "logging_path = path_now + 'results.csv'\n",
    "\n",
    "for k, v in params_sst_sn.items():\n",
    "    params_to_save[k] = v\n",
    "\n",
    "with open(path_now + 'params.json', 'w') as f:\n",
    "    json.dump(params_to_save, f, indent=4)\n",
    "\n",
    "highest_val_acc = 0\n",
    "\n",
    "# normal training loop\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    if epoch == 30:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 5e-5\n",
    "            print('updated learning rate')\n",
    "    model.train()\n",
    "    train_loss, train_acc = process_train_eval(\n",
    "        model, train_loader, criterion, optimizer\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc = process_train_eval(model, val_loader, criterion)\n",
    "\n",
    "    # save metrics\n",
    "    save_train_metrics(\n",
    "        epoch,\n",
    "        train_loss,\n",
    "        train_acc,\n",
    "        val_loss,\n",
    "        val_acc,\n",
    "        path=logging_path,\n",
    "    )\n",
    "\n",
    "\n",
    "    if val_acc > highest_val_acc:\n",
    "        highest_val_acc = val_acc\n",
    "        _path = path_now + f\"acc{val_acc:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), _path)\n",
    "\n",
    "    print(\n",
    "        f\"Training:   [Epoch {epoch:2d}, Loss: {train_loss:8.6f}, Acc: {train_acc:.6f}]\"\n",
    "    )\n",
    "    print(f\"Evaluation: [Epoch {epoch:2d}, Loss: {val_loss:8.6f}, Acc: {val_acc:.6f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b1daf-c25b-44f0-ad2b-b33f1c771c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d49251-6176-46ed-8a3c-a09376a7e360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
